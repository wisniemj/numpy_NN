{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset import from mnist\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the 28x28 matrix into 784 row for each picture, transpose so each pixel represents a row\n",
    "x_train_flattened = x_train.reshape(-1, x_train.shape[0]) # -1 to unpivot 28x28, x_train.shape[0] = 60,000\n",
    "x_test_flattened = x_test.reshape(-1, x_test.shape[0])\n",
    "\n",
    "x_train_flattened = x_train_flattened / 255.\n",
    "x_test_flattened = x_test_flattened / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def init_params():\n",
    "    \"\"\"\n",
    "    Initialize parameters W1, B1, W2, B2\n",
    "    \"\"\"\n",
    "    W1 = np.random.rand(10, 784) - 0.5 # subtract 0.5 to get numbers between -0.5 and 0.5\n",
    "    b1 = np.random.rand(10, 1)\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1)\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Reactivation Linear Unit\n",
    "    If X > 0, X\n",
    "    If X <= 0, 0\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)  # compares 0 to Z elementwise, so compares 0 to each element in matrix Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    e^[M] / sum(e^[M])\n",
    "        where M is a matrix and this\n",
    "        operation is performed elementwise\n",
    "    \"\"\"\n",
    "    A = np.exp(Z - np.max(Z)) / np.sum(np.exp(Z - np.max(Z)), axis=0)\n",
    "    return A\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    \n",
    "    # first layer\n",
    "    Z1 = W1.dot(X) + b1  # first layer calculation -> weights dotproduct input X, add bias\n",
    "    A1 = ReLU(Z1)  # activation function for first layer\n",
    "    \n",
    "    # second layer\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_y = np.zeros((Y.size, Y.max() + 1))  # this creates a matrix of size Y rows by the max label + 1, so if 9 -> 10 columns total because there is 10 labels in 0-9\n",
    "    one_hot_y[np.arange(Y.size), Y] = 1  # creates an M x Features matrix and assigns 1 to the correct label -> for each row, find the original label and assign 1 to that row, column\n",
    "    one_hot_y = one_hot_y.T  # transpose so each label is a row\n",
    "\n",
    "    return one_hot_y\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0  # derivative of 1 = 1, 0 = 0 lol\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    \n",
    "    # encode labels\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "\n",
    "    # from output layer to last hidden layer\n",
    "    dZ2 = 2*(A2 - one_hot_Y)\n",
    "    dW2 = 1 / m * (dZ2.dot(A1.T))\n",
    "    db2 = 1 / m * np.sum(dZ2, 1)\n",
    "\n",
    "    # from last hidden layer to first hidden layer\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "    dW1 = 1 / m * (dZ1.dot(X.T))\n",
    "    db1 = 1 / m * np.sum(dZ1, 1)\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    \"\"\"\n",
    "    Take original values, subtract some learning rate * derivatives to arrive at a new value\n",
    "    \"\"\"\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * np.reshape(db1, (10, 1))\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * np.reshape(db2, (10, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    \n",
    "    # initialize parameters\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "\n",
    "    # iterate through each epoch\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "0.10051666666666667\n",
      "Iteration: 10\n",
      "0.10341666666666667\n",
      "Iteration: 20\n",
      "0.10535\n",
      "Iteration: 30\n",
      "0.10558333333333333\n",
      "Iteration: 40\n",
      "0.10695\n",
      "Iteration: 50\n",
      "0.10653333333333333\n",
      "Iteration: 60\n",
      "0.10636666666666666\n",
      "Iteration: 70\n",
      "0.10716666666666666\n",
      "Iteration: 80\n",
      "0.10788333333333333\n",
      "Iteration: 90\n",
      "0.10913333333333333\n",
      "Iteration: 100\n",
      "0.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m gradient_descent(x_train_flattened, y_train, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 95\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, Y, iterations, alpha)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m     94\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m forward_prop(W1, b1, W2, b2, X)\n\u001b[1;32m---> 95\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[0;32m     96\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[39], line 67\u001b[0m, in \u001b[0;36mback_prop\u001b[1;34m(Z1, A1, Z2, A2, W1, W2, X, Y)\u001b[0m\n\u001b[0;32m     65\u001b[0m dZ1 \u001b[38;5;241m=\u001b[39m W2\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(dZ2) \u001b[38;5;241m*\u001b[39m deriv_ReLU(Z1)\n\u001b[0;32m     66\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m \u001b[38;5;241m*\u001b[39m (dZ1\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m---> 67\u001b[0m db1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ1, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dW1, db1, dW2, db2\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\EV-04\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m   2325\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mc:\\Users\\EV-04\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(x_train_flattened, y_train, 500, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
